"""
Fact-Check å·¥å…· - æ™ºèƒ½æ‘˜è¦å’ŒéªŒè¯
ç»“åˆçŸ¥è¯†åº“æ£€ç´¢å’Œå¯é€‰çš„ç½‘ç»œæœç´¢ï¼Œç”Ÿæˆæ€»ç»“æ€§æ–‡æœ¬
"""

import os
from langchain_community.llms import Tongyi
from dotenv import load_dotenv

# å¯¼å…¥ç»Ÿä¸€çš„Promptç®¡ç†æ¨¡å—
from prompts import Prompts

load_dotenv()

def summarize_fact_check(question, retrieved_docs, ai_answer, language="English"):
    """
    å¯¹ Fact-Check å†…å®¹è¿›è¡Œæ™ºèƒ½æ‘˜è¦
    
    ä½¿ç”¨Promptsæ¨¡å—ç»Ÿä¸€ç®¡ç†çš„Fact-Check prompt
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        ai_answer: AI çš„å›ç­”
        language: è¯­è¨€ï¼ˆEnglish/Portugueseï¼‰
    
    Returns:
        str: æ€»ç»“æ€§æ–‡æœ¬
    """
    # æå–æ–‡æ¡£å†…å®¹
    doc_contents = []
    sources = []
    
    for i, doc in enumerate(retrieved_docs[:3], 1):  # æœ€å¤šä½¿ç”¨3ä¸ªæ–‡æ¡£
        content = doc.page_content[:500]  # æ¯ä¸ªæ–‡æ¡£æœ€å¤š500å­—ç¬¦
        source = doc.metadata.get('source_file', 'Unknown')
        page = doc.metadata.get('page', 'N/A')
        
        doc_contents.append(f"[Source {i}: {source}, Page {page}]\n{content}")
        sources.append(f"{source} (p.{page})")
    
    combined_docs = "\n\n".join(doc_contents)
    
    # ä½¿ç”¨Promptsæ¨¡å—ç”ŸæˆFact-Checkæ‘˜è¦prompt
    prompt = Prompts.get_fact_check_summary_prompt(
        question=question,
        ai_answer=ai_answer,
        doc_contents=combined_docs,
        language=language
    )
    
    # ä½¿ç”¨ Qwen LLM ç”Ÿæˆæ‘˜è¦
    try:
        api_key = os.getenv("DASHSCOPE_API_KEY")
        llm = Tongyi(
            model_name=os.getenv("QWEN_MODEL_NAME", "qwen-turbo"),
            temperature=0.3,  # è¾ƒä½æ¸©åº¦ï¼Œç¡®ä¿äº‹å®æ€§
            dashscope_api_key=api_key
        )
        
        summary = llm.invoke(prompt)
        
        # æ·»åŠ æ¥æºå¼•ç”¨
        if language == "Portuguese":
            source_text = f"\n\nğŸ“š **Fontes:** {', '.join(sources)}"
        else:
            source_text = f"\n\nğŸ“š **Sources:** {', '.join(sources)}"
        
        return summary.strip() + source_text
    
    except Exception as e:
        print(f"[Fact-Check] æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
        # é™çº§ï¼šè¿”å›ç®€åŒ–çš„æ–‡æ¡£å†…å®¹
        if language == "Portuguese":
            return f"ğŸ“„ InformaÃ§Ã£o extraÃ­da dos documentos:\n\n{retrieved_docs[0].page_content[:200]}...\n\nğŸ“š Fonte: {sources[0]}"
        else:
            return f"ğŸ“„ Information from documents:\n\n{retrieved_docs[0].page_content[:200]}...\n\nğŸ“š Source: {sources[0]}"


def optimize_search_query(question, retrieved_docs):
    """
    åŸºäºç”¨æˆ·é—®é¢˜å’Œ RAG æ£€ç´¢å†…å®¹ä¼˜åŒ–æœç´¢æŸ¥è¯¢
    
    Args:
        question: ç”¨æˆ·åŸå§‹é—®é¢˜
        retrieved_docs: RAG æ£€ç´¢åˆ°çš„æ–‡æ¡£
    
    Returns:
        str: ä¼˜åŒ–åçš„æœç´¢æŸ¥è¯¢
    """
    # ä» RAG æ–‡æ¡£ä¸­æå–å…³é”®æ¦‚å¿µ
    rag_keywords = set()
    for doc in retrieved_docs[:2]:  # åªçœ‹å‰2ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
        content = doc.page_content.lower()
        # æå–å…³é”®ç”Ÿç‰©å­¦/ä¿æŠ¤ç›¸å…³è¯æ±‡
        bio_keywords = ['seabird', 'petrel', 'bird', 'endemic', 'madeira', 'conservation', 
                        'endangered', 'breeding', 'nesting', 'habitat', 'species', 'population']
        for keyword in bio_keywords:
            if keyword in content:
                rag_keywords.add(keyword)
    
    # æ„å»ºç²¾å‡†æœç´¢æŸ¥è¯¢
    base_query = "Zino's Petrel"
    
    # æ·»åŠ ç›¸å…³ä¸Šä¸‹æ–‡å…³é”®è¯
    if 'conservation' in rag_keywords or 'endangered' in rag_keywords:
        base_query += " conservation status"
    elif 'breeding' in rag_keywords or 'nesting' in rag_keywords:
        base_query += " breeding habitat"
    elif 'madeira' in rag_keywords:
        base_query += " Madeira island"
    else:
        base_query += " seabird biology"
    
    # æ·»åŠ è‹±æ–‡å…³é”®è¯ç¡®ä¿æœç´¢è´¨é‡
    base_query += " bird species"
    
    return base_query


def filter_search_results(results, question):
    """
    æ™ºèƒ½è¿‡æ»¤æœç´¢ç»“æœï¼Œæ’é™¤æ— å…³å†…å®¹
    
    Args:
        results: åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
        question: ç”¨æˆ·é—®é¢˜
    
    Returns:
        list: è¿‡æ»¤åçš„ç›¸å…³ç»“æœ
    """
    filtered = []
    
    # ç›¸å…³å…³é”®è¯ï¼ˆç”Ÿç‰©å­¦/ä¿æŠ¤ç›¸å…³ï¼‰
    relevant_keywords = [
        'petrel', 'bird', 'seabird', 'species', 'madeira', 'conservation', 
        'endangered', 'breeding', 'habitat', 'ornithology', 'wildlife',
        'pterodroma', 'freira', 'endemic', 'biodiversity'
    ]
    
    # æ— å…³å…³é”®è¯ï¼ˆæŠ€æœ¯/ç¼–ç¨‹ç›¸å…³ï¼‰
    irrelevant_keywords = [
        'framework', 'programming', 'code', 'software', 'api', 'rust',
        'ç¼–ç¨‹', 'æ¡†æ¶', 'å¼€å‘', 'ä»£ç ', 'github', 'npm', 'cargo'
    ]
    
    for result in results:
        title = result.get('title', '').lower()
        body = result.get('body', '').lower()
        combined = title + ' ' + body
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— å…³å…³é”®è¯
        has_irrelevant = any(keyword in combined for keyword in irrelevant_keywords)
        if has_irrelevant:
            print(f"[Fact-Check] è¿‡æ»¤æ— å…³ç»“æœ: {result.get('title', 'Unknown')[:50]}...")
            continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³å…³é”®è¯
        has_relevant = any(keyword in combined for keyword in relevant_keywords)
        if has_relevant:
            filtered.append(result)
    
    return filtered


def web_search_supplement(question, retrieved_docs=None, language="English"):
    """
    æ™ºèƒ½ç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯
    æ”¯æŒ DuckDuckGoï¼ˆå…è´¹ï¼‰å’Œ Tavilyï¼ˆéœ€ API Keyï¼‰
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        retrieved_docs: RAG æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆç”¨äºä¼˜åŒ–æœç´¢æŸ¥è¯¢ï¼‰
        language: è¯­è¨€
    
    Returns:
        str: ç½‘ç»œæœç´¢ç»“æœæ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    """
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
    use_web_search = os.getenv("USE_WEB_SEARCH", "false").lower() == "true"
    
    if not use_web_search:
        return None
    
    # ä¼˜åŒ–æœç´¢æŸ¥è¯¢ï¼ˆåŸºäº RAG ä¸Šä¸‹æ–‡ï¼‰
    if retrieved_docs and len(retrieved_docs) > 0:
        optimized_query = optimize_search_query(question, retrieved_docs)
        print(f"[Fact-Check] ä¼˜åŒ–æœç´¢æŸ¥è¯¢: {optimized_query}")
    else:
        optimized_query = f"Zino's Petrel {question} bird species"
    
    # è·å–æœç´¢æä¾›å•†ï¼ˆé»˜è®¤ duckduckgoï¼‰
    provider = os.getenv("WEB_SEARCH_PROVIDER", "duckduckgo").lower()
    
    # æ–¹æ¡ˆ 1: DuckDuckGoï¼ˆå®Œå…¨å…è´¹ï¼Œæ— éœ€ API Keyï¼‰
    results = []  # åˆå§‹åŒ– results å˜é‡
    
    if provider == "duckduckgo":
        try:
            from ddgs import DDGS
            
            # ä½¿ç”¨æ–°ç‰ˆ APIï¼ˆæ— éœ€ context managerï¼‰
            ddgs = DDGS()
            # æ–°ç‰ˆ APIï¼šå‚æ•°åæ˜¯ query è€Œä¸æ˜¯ keywords
            raw_results = list(ddgs.text(
                query=optimized_query,
                max_results=5  # å¤šè·å–ä¸€äº›ç»“æœï¼Œåç»­è¿‡æ»¤
            ))
            
            # æ™ºèƒ½è¿‡æ»¤ç»“æœ
            results = filter_search_results(raw_results, question)
            print(f"[Fact-Check] åŸå§‹ç»“æœ: {len(raw_results)} â†’ è¿‡æ»¤å: {len(results)}")
            
            if results:
                if language == "Portuguese":
                    summary = "ğŸŒ **InformaÃ§Ã£o da Internet:**\n\n"
                else:
                    summary = "ğŸŒ **Internet Information:**\n\n"
                
                # åªæ˜¾ç¤ºå‰2ä¸ªæœ€ç›¸å…³çš„ç»“æœ
                for i, result in enumerate(results[:2], 1):
                    title = result.get('title', 'Unknown')
                    body = result.get('body', '')[:150]
                    url = result.get('href', '')
                    
                    summary += f"{i}. **{title}**\n   {body}...\n   ğŸ”— {url}\n\n"
                
                return summary.strip()
        
        except ImportError:
            print("[Fact-Check] DDGS æœªå®‰è£…ï¼Œè¿è¡Œ: pip install ddgs")
        except Exception as e:
            print(f"[Fact-Check] DuckDuckGo æœç´¢å¤±è´¥: {str(e)}")
            print(f"[Fact-Check] å°è¯•é™çº§åˆ° Tavily...")
    
    # æ–¹æ¡ˆ 2: Tavilyï¼ˆéœ€è¦ API Keyï¼Œ1000 æ¬¡/æœˆå…è´¹ï¼‰
    # å¦‚æœ DuckDuckGo å¤±è´¥æˆ–æä¾›å•†è®¾ç½®ä¸º tavilyï¼Œå°è¯• Tavily
    if provider == "tavily" or (provider == "duckduckgo" and len(results) == 0):
        try:
            tavily_key = os.getenv("TAVILY_API_KEY")
            if tavily_key and tavily_key != "tvly-your-api-key":
                from tavily import TavilyClient
                
                client = TavilyClient(api_key=tavily_key)
                response = client.search(
                    query=f"Zino's Petrel {question}",
                    max_results=2,
                    search_depth="basic"
                )
                
                if response and 'results' in response:
                    results = response['results'][:2]
                    
                    if language == "Portuguese":
                        summary = "ğŸŒ **InformaÃ§Ã£o da Internet:**\n\n"
                    else:
                        summary = "ğŸŒ **Internet Information:**\n\n"
                    
                    for i, result in enumerate(results, 1):
                        title = result.get('title', 'Unknown')
                        content = result.get('content', '')[:150]
                        url = result.get('url', '')
                        
                        summary += f"{i}. **{title}**\n   {content}...\n   ğŸ”— {url}\n\n"
                    
                    return summary.strip()
        
        except ImportError:
            print("[Fact-Check] Tavily æœªå®‰è£…ï¼Œè¿è¡Œ: pip install tavily-python")
        except Exception as e:
            print(f"[Fact-Check] Tavily æœç´¢å¤±è´¥: {str(e)}")
    
    return None


def generate_fact_check_content(question, retrieved_docs, ai_answer, language="English"):
    """
    ç”Ÿæˆå®Œæ•´çš„ Fact-Check å†…å®¹ï¼ˆæ™ºèƒ½ä¼˜åŒ–ç‰ˆï¼‰
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£
        ai_answer: AI å›ç­”
        language: è¯­è¨€
    
    Returns:
        str: HTML æ ¼å¼çš„ Fact-Check å†…å®¹
    """
    # 1. ç”ŸæˆçŸ¥è¯†åº“æ‘˜è¦
    kb_summary = summarize_fact_check(question, retrieved_docs, ai_answer, language)
    
    # 2. å¯é€‰ï¼šæ™ºèƒ½ç½‘ç»œæœç´¢è¡¥å……ï¼ˆä¼ é€’ RAG æ–‡æ¡£ç”¨äºä¼˜åŒ–æœç´¢æŸ¥è¯¢ï¼‰
    web_summary = web_search_supplement(
        question=question, 
        retrieved_docs=retrieved_docs,  # ä¼ é€’ RAG ä¸Šä¸‹æ–‡ä¼˜åŒ–æœç´¢
        language=language
    )
    
    # 3. ç»„åˆå†…å®¹
    if language == "Portuguese":
        header = "ğŸ“‹ **VerificaÃ§Ã£o de Factos Baseada em Conhecimento CientÃ­fico**\n\n"
    else:
        header = "ğŸ“‹ **Fact-Check Based on Scientific Knowledge**\n\n"
    
    content = header + kb_summary
    
    if web_summary:
        content += f"\n\n---\n\n{web_summary}"
    
    return content

