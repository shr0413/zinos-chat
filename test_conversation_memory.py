"""
æµ‹è¯•å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†åŠŸèƒ½
éªŒè¯ ConversationalRetrievalChain æ˜¯å¦èƒ½è®°ä½ä¹‹å‰çš„å¯¹è¯
"""

import os
from dotenv import load_dotenv
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
from langchain_community.llms import Tongyi
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_chroma import Chroma

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def test_conversation_memory():
    """æµ‹è¯•å¯¹è¯è®°å¿†åŠŸèƒ½"""
    
    print("=" * 60)
    print("ğŸ§ª å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åˆå§‹åŒ–é…ç½®
    dashscope_key = os.getenv("DASHSCOPE_API_KEY")
    persist_directory = "db5_qwen"
    
    print(f"\nâœ… API Key: {dashscope_key[:10]}...")
    print(f"âœ… å‘é‡åº“: {persist_directory}")
    
    # åŠ è½½å‘é‡æ•°æ®åº“
    print("\n[1/4] åŠ è½½å‘é‡æ•°æ®åº“...")
    embeddings = DashScopeEmbeddings(
        model=os.getenv("QWEN_EMBEDDING_MODEL", "text-embedding-v3"),
        dashscope_api_key=dashscope_key
    )
    
    vectordb = Chroma(
        embedding_function=embeddings,
        persist_directory=persist_directory,
        collection_name="zinos_petrel_knowledge"
    )
    print("âœ… å‘é‡æ•°æ®åº“å·²åŠ è½½")
    
    # åˆ›å»º LLM
    print("\n[2/4] åˆ›å»º LLM...")
    model = Tongyi(
        model_name=os.getenv("QWEN_MODEL_NAME", "qwen-turbo"),
        temperature=0,
        dashscope_api_key=dashscope_key
    )
    print("âœ… LLM å·²åˆ›å»º")
    
    # åˆ›å»º Retriever
    print("\n[3/4] åˆ›å»º Retriever...")
    retriever = vectordb.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 3,
            "fetch_k": 9,
            "lambda_mult": 0.3
        }
    )
    print("âœ… Retriever å·²åˆ›å»º")
    
    # åˆ›å»ºå¯¹è¯è®°å¿†
    print("\n[4/4] åˆ›å»ºå¯¹è¯è®°å¿†...")
    memory = ConversationBufferWindowMemory(
        k=5,
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    print("âœ… Memory å·²åˆ›å»ºï¼ˆä¿ç•™æœ€è¿‘5è½®å¯¹è¯ï¼‰")
    
    # åˆ›å»º ConversationalRetrievalChain
    print("\nåˆ›å»º ConversationalRetrievalChain...")
    
    base_prompt = """You are Fred, a friendly and knowledgeable Zino's Petrel (Freira da Madeira). 
You are a seabird living in Madeira, Portugal. You love to share information about your species, 
habitat, and conservation. Be warm, personal, and enthusiastic!"""
    
    combine_docs_prompt = PromptTemplate(
        template=f"""
{base_prompt}

Use the following context to answer the question. If you don't know the answer based on the context, say so honestly.

Context:
{{context}}

Question: {{question}}

Answer:""",
        input_variables=["context", "question"]
    )
    
    chain = ConversationalRetrievalChain.from_llm(
        llm=model,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": combine_docs_prompt},
        return_source_documents=True,
        verbose=False
    )
    print("âœ… Chain å·²åˆ›å»º")
    
    # æµ‹è¯•åœºæ™¯ï¼šä¸Šä¸‹æ–‡å¯¹è¯
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•åœºæ™¯ï¼šä¸Šä¸‹æ–‡å¯¹è¯ç†è§£")
    print("=" * 60)
    
    test_conversations = [
        {
            "question": "Where do you live?",
            "expected_keywords": ["Madeira", "mountains", "island", "Portugal"],
            "context_check": None
        },
        {
            "question": "How high is it there?",
            "expected_keywords": ["altitude", "elevation", "meters", "mountain"],
            "context_check": "èƒ½å¦ç†è§£ 'it' æŒ‡ä»£ Madeira æˆ–æ –æ¯åœ°"
        },
        {
            "question": "Is it cold at night?",
            "expected_keywords": ["temperature", "cold", "night", "climate"],
            "context_check": "èƒ½å¦ç†è§£ 'it' æŒ‡ä»£æ –æ¯åœ°çš„æ¸©åº¦"
        },
        {
            "question": "What do you eat?",
            "expected_keywords": ["fish", "squid", "food", "prey"],
            "context_check": None
        },
        {
            "question": "How do you catch them?",
            "expected_keywords": ["catch", "hunt", "dive", "sea"],
            "context_check": "èƒ½å¦ç†è§£ 'them' æŒ‡ä»£é£Ÿç‰©ï¼ˆfish/squidï¼‰"
        }
    ]
    
    results = []
    
    for i, test in enumerate(test_conversations, 1):
        print(f"\n{'â”€' * 60}")
        print(f"æµ‹è¯• {i}/{len(test_conversations)}")
        print(f"{'â”€' * 60}")
        print(f"ğŸ“ é—®é¢˜: {test['question']}")
        if test['context_check']:
            print(f"ğŸ¯ ä¸Šä¸‹æ–‡æ£€æŸ¥: {test['context_check']}")
        
        # è°ƒç”¨ chain
        try:
            result = chain.invoke({"question": test['question']})
            answer = result.get("answer", "")
            
            print(f"\nğŸ’¬ Fred çš„å›ç­”:\n{answer}\n")
            
            # å…³é”®è¯æ£€æŸ¥
            found_keywords = []
            for keyword in test['expected_keywords']:
                if keyword.lower() in answer.lower():
                    found_keywords.append(keyword)
            
            coverage = len(found_keywords) / len(test['expected_keywords']) * 100
            
            print(f"ğŸ“Š å…³é”®è¯è¦†ç›–ç‡: {coverage:.1f}%")
            print(f"   æ‰¾åˆ°: {', '.join(found_keywords) if found_keywords else 'æ— '}")
            
            # è¯„ä¼°
            if test['context_check']:
                # éœ€è¦äººå·¥åˆ¤æ–­ä¸Šä¸‹æ–‡æ˜¯å¦è¢«ç†è§£
                print(f"\nâ“ è¯·æ‰‹åŠ¨åˆ¤æ–­: {test['context_check']}")
                passed = coverage >= 25  # å®½æ¾æ ‡å‡†ï¼Œä¸»è¦çœ‹ä¸Šä¸‹æ–‡ç†è§£
            else:
                passed = coverage >= 40
            
            results.append({
                "question": test['question'],
                "passed": passed,
                "coverage": coverage,
                "context_check": test['context_check']
            })
            
            if passed:
                print("âœ… æµ‹è¯•é€šè¿‡")
            else:
                print("âš ï¸ è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥")
                
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")
            results.append({
                "question": test['question'],
                "passed": False,
                "coverage": 0,
                "error": str(e)
            })
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    passed_count = sum(1 for r in results if r['passed'])
    total_count = len(results)
    
    print(f"\nâœ… é€šè¿‡æµ‹è¯•: {passed_count}/{total_count} ({passed_count/total_count*100:.1f}%)")
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    print("\n" + "=" * 60)
    print("ğŸ’­ å½“å‰å¯¹è¯å†å² (Memory å­˜å‚¨)")
    print("=" * 60)
    
    print(f"\næ€»å¯¹è¯è½®æ•°: {len(memory.chat_memory.messages) // 2}")
    print(f"Memory çª—å£å¤§å°: æœ€è¿‘ {memory.k} è½®")
    
    if len(memory.chat_memory.messages) > 0:
        print("\nå¯¹è¯è®°å½•:")
        for i, msg in enumerate(memory.chat_memory.messages):
            role = "ğŸ‘¤ ç”¨æˆ·" if msg.type == "human" else "ğŸ¦ Fred"
            content = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
            print(f"  {role}: {content}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    
    print("\nğŸ’¡ éªŒè¯è¦ç‚¹:")
    print("  1. ç¬¬2ä¸ªé—®é¢˜èƒ½å¦ç†è§£ 'it there' æŒ‡ä»£æ –æ¯åœ°")
    print("  2. ç¬¬3ä¸ªé—®é¢˜èƒ½å¦ç†è§£ 'it' æŒ‡ä»£æ –æ¯åœ°çš„æ¸©åº¦")
    print("  3. ç¬¬5ä¸ªé—®é¢˜èƒ½å¦ç†è§£ 'them' æŒ‡ä»£é£Ÿç‰©")
    
    return results

if __name__ == "__main__":
    test_conversation_memory()

