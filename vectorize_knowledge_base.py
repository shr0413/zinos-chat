"""
RAG å‘é‡åŒ–è„šæœ¬ - Qwen å…¨å®¶æ¡¶ç‰ˆæœ¬
å°† Zino's Petrel æ–‡çŒ®åº“å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° ChromaDB

ä½¿ç”¨æ–¹æ³•:
    python vectorize_knowledge_base.py

åŠŸèƒ½:
    - æ‰¹é‡å¤„ç† PDF æ–‡ä»¶
    - ä¼˜åŒ–çš„æ–‡æ¡£åˆ†å‰²ï¼ˆchunk_overlap=200ï¼‰
    - Qwen Embedding (text-embedding-v3)
    - è¿›åº¦è¿½è¸ªå’Œé”™è¯¯å¤„ç†
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from tqdm import tqdm
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_chroma import Chroma

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
PDF_FOLDER = "Zino's Petrel"
VECTOR_DB_PATH = "db5_qwen"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200  # 20% é‡å ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
# ä»ç¯å¢ƒå˜é‡è¯»å– Embedding æ¨¡å‹ï¼ˆä¸ rag_utils.py ä¿æŒä¸€è‡´ï¼‰
EMBEDDING_MODEL = os.getenv("QWEN_EMBEDDING_MODEL", "text-embedding-v3")

def get_pdf_files(folder_path):
    """è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰ PDF æ–‡ä»¶"""
    pdf_path = Path(folder_path)
    if not pdf_path.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶å¤¹ '{folder_path}' ä¸å­˜åœ¨")
        sys.exit(1)
    
    pdf_files = list(pdf_path.glob("*.pdf"))
    if not pdf_files:
        print(f"âš ï¸  è­¦å‘Š: æ–‡ä»¶å¤¹ '{folder_path}' ä¸­æ²¡æœ‰ PDF æ–‡ä»¶")
        sys.exit(1)
    
    return pdf_files

def load_and_split_pdf(pdf_path, text_splitter):
    """åŠ è½½å¹¶åˆ†å‰²å•ä¸ª PDF æ–‡ä»¶"""
    try:
        loader = PyPDFLoader(str(pdf_path))
        pages = loader.load()
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ å…ƒæ•°æ®
        for i, page in enumerate(pages):
            page.metadata.update({
                "source_file": pdf_path.name,
                "page": i + 1,
                "total_pages": len(pages)
            })
        
        # åˆ†å‰²æ–‡æ¡£
        chunks = text_splitter.split_documents(pages)
        return chunks, None
    
    except Exception as e:
        return None, str(e)

def vectorize_documents(pdf_files, embeddings, text_splitter):
    """å‘é‡åŒ–æ‰€æœ‰æ–‡æ¡£"""
    all_chunks = []
    failed_files = []
    
    print(f"\nğŸ“š å¼€å§‹å¤„ç† {len(pdf_files)} ä¸ª PDF æ–‡ä»¶...\n")
    
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
    for pdf_file in tqdm(pdf_files, desc="å¤„ç† PDF", unit="æ–‡ä»¶"):
        chunks, error = load_and_split_pdf(pdf_file, text_splitter)
        
        if error:
            failed_files.append((pdf_file.name, error))
            tqdm.write(f"âŒ å¤±è´¥: {pdf_file.name} - {error}")
        else:
            all_chunks.extend(chunks)
            tqdm.write(f"âœ… æˆåŠŸ: {pdf_file.name} ({len(chunks)} chunks)")
    
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"  - æˆåŠŸ: {len(pdf_files) - len(failed_files)} ä¸ªæ–‡ä»¶")
    print(f"  - å¤±è´¥: {len(failed_files)} ä¸ªæ–‡ä»¶")
    print(f"  - æ€»å—æ•°: {len(all_chunks)} chunks")
    
    if failed_files:
        print(f"\nâš ï¸  å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
        for filename, error in failed_files:
            print(f"  - {filename}: {error}")
    
    return all_chunks

def create_vector_store(chunks, embeddings, persist_directory):
    """åˆ›å»ºå¹¶æŒä¹…åŒ–å‘é‡æ•°æ®åº“"""
    print(f"\nğŸ”„ åˆ›å»ºå‘é‡æ•°æ®åº“...")
    print(f"  - å‘é‡åº“è·¯å¾„: {persist_directory}")
    print(f"  - åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL}")
    print(f"  - æ–‡æ¡£å—æ•°é‡: {len(chunks)}")
    
    try:
        # æ¸…ç©ºæ—§æ•°æ®åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if Path(persist_directory).exists():
            import shutil
            shutil.rmtree(persist_directory)
            print(f"  - å·²æ¸…ç©ºæ—§æ•°æ®åº“")
        
        # åˆ†æ‰¹å¤„ç†å‘é‡åŒ–ï¼ˆDashScope é™åˆ¶ï¼šbatch_size â‰¤ 10ï¼‰
        batch_size = 10
        vectordb = None
        
        for i in tqdm(range(0, len(chunks), batch_size), desc="å‘é‡åŒ–", unit="æ‰¹æ¬¡"):
            batch = chunks[i:i + batch_size]
            
            if vectordb is None:
                # é¦–æ¬¡åˆ›å»º
                vectordb = Chroma.from_documents(
                    documents=batch,
                    embedding=embeddings,
                    persist_directory=persist_directory,
                    collection_name="zinos_petrel_knowledge"
                )
            else:
                # è¿½åŠ åˆ°ç°æœ‰æ•°æ®åº“
                vectordb.add_documents(batch)
        
        print(f"\nâœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ!")
        return vectordb
    
    except Exception as e:
        print(f"\nâŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def test_retrieval(vectordb):
    """æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
    print(f"\nğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
    
    test_queries = [
        "What is Zino's Petrel?",
        "Where does Zino's Petrel nest?",
        "What does Zino's Petrel eat?"
    ]
    
    for query in test_queries:
        print(f"\nğŸ“ æŸ¥è¯¢: '{query}'")
        results = vectordb.similarity_search(query, k=2)
        
        for i, doc in enumerate(results, 1):
            print(f"\n  ç»“æœ {i}:")
            print(f"    - æ¥æº: {doc.metadata.get('source_file', 'Unknown')}")
            print(f"    - é¡µç : {doc.metadata.get('page', 'N/A')}")
            print(f"    - å†…å®¹é¢„è§ˆ: {doc.page_content[:150]}...")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ“š RAG å‘é‡åŒ–è„šæœ¬ - Qwen å…¨å®¶æ¡¶ç‰ˆæœ¬")
    print("=" * 60)
    
    # 1. æ£€æŸ¥ API Key
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° DASHSCOPE_API_KEY")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® API Key")
        sys.exit(1)
    
    print(f"âœ… API Key å·²é…ç½®")
    
    # 2. è·å– PDF æ–‡ä»¶åˆ—è¡¨
    pdf_files = get_pdf_files(PDF_FOLDER)
    print(f"âœ… æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
    
    # 3. åˆå§‹åŒ– Embeddings
    print(f"\nğŸ”§ åˆå§‹åŒ– Embedding æ¨¡å‹...")
    embeddings = DashScopeEmbeddings(
        model=EMBEDDING_MODEL,
        dashscope_api_key=api_key
    )
    print(f"âœ… ä½¿ç”¨æ¨¡å‹: {EMBEDDING_MODEL}")
    
    # 4. åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    print(f"âœ… æ–‡æœ¬åˆ†å‰²é…ç½®: chunk_size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}")
    
    # 5. å‘é‡åŒ–æ–‡æ¡£
    chunks = vectorize_documents(pdf_files, embeddings, text_splitter)
    
    if not chunks:
        print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡æ¡£")
        sys.exit(1)
    
    # 6. åˆ›å»ºå‘é‡æ•°æ®åº“
    vectordb = create_vector_store(chunks, embeddings, VECTOR_DB_PATH)
    
    # 7. æµ‹è¯•æ£€ç´¢
    test_retrieval(vectordb)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ å‘é‡åŒ–å®Œæˆ!")
    print("=" * 60)
    print(f"\nğŸ“ å‘é‡åº“ä½ç½®: {VECTOR_DB_PATH}")
    print(f"ğŸ“Š æ€»æ–‡æ¡£å—æ•°: {len(chunks)}")
    print(f"\nä¸‹ä¸€æ­¥: è¿è¡Œ 'streamlit run main.py' å¼€å§‹ä½¿ç”¨!")

if __name__ == "__main__":
    main()

